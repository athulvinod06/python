{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5py1NhTuSpSjtaFiAJQHG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/athulvinod06/python/blob/main/Copy_of_Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpQTaWvMBcmn",
        "outputId": "4455bff3-60c8-490b-d022-481f6ce9d4f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import twitter_samples\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "np.random.seed(1)\n",
        "rand= np.random.randint(0, 5000, 5)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                                                \n",
        "                                                \n",
        "                                                \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Positive Tweets ::')\n",
        " \n",
        "print(\"\\033[92m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OL1VYJ6WFHmp",
        "outputId": "047656bd-cf6d-4eef-a844-1399a9aba33e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive Tweets ::\n",
            "\u001b[92m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in rand:\n",
        "      print(all_positive_tweets[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NlP5w12Fmfb",
        "outputId": "51478dd8-6113-4bb4-feeb-63671244af02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!\n",
            "@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!\n",
            "@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!\n",
            "@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!\n",
            "@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Negative Tweets ::')\n",
        "print('\\033[91m')\n",
        "for i in rand:\n",
        "  print(all_negative_tweets[1])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXBz1JRVFNHj",
        "outputId": "7c59eec1-8fbe-4ef0-daa2-171b8ab266a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Negative Tweets ::\n",
            "\u001b[91m\n",
            "Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(\n",
            "Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(\n",
            "Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(\n",
            "Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(\n",
            "Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def process_tweet(tweet):\n",
        "  stemmer = PorterStemmer() \n",
        "  stopwords_english = stopwords.words('english')\n",
        "\n",
        "  # remove the stock market tickers\n",
        "  tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "\n",
        "  # remove the old styles retweet text 'RT'\n",
        "  tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "\n",
        "  # remove the hyperlinks\n",
        "  tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "\n",
        "  # remove the # symbol\n",
        "  tweet = re.sub(r'#', '', tweet)\n",
        "\n",
        "  # Tokenize the tweet\n",
        "  tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
        "  tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "  tweet_clean = []\n",
        "\n",
        "  # removing stopwords and punctuation\n",
        "  for word in tweet_tokens:\n",
        "    if (word not in stopwords_english and\n",
        "        word not in string.punctuation):\n",
        "      stem_word = stemmer.stem(word)    #stemming\n",
        "      tweet_clean.append(stem_word)\n",
        "\n",
        "  return tweet_clean"
      ],
      "metadata": {
        "id": "HgwnpF8II3fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet = all_positive_tweets[np.random.randint(0,5000)]\n",
        "print('Raw tweet : \\n', tweet)\n",
        "tweet = process_tweet(tweet)\n",
        "print('After processing the tweet : \\n' , tweet)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omWy4914X2hT",
        "outputId": "46dfb7a5-e287-4b84-8470-b565f8109e42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw tweet : \n",
            " @WforWoman 5. Over 20 W kurtas! And my Mom has about half the number I have :D #WSaleLove\n",
            "After processing the tweet : \n",
            " ['5', '20', 'w', 'kurta', 'mom', 'half', 'number', ':d', 'wsalelov']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_tweets(tweets, ys):\n",
        "  ys_list = np.squeeze(ys).tolist()\n",
        "  freqs ={}\n",
        "\n",
        "  for y, tweet in zip(ys_list, tweets):\n",
        "    for word in process_tweet(tweet):\n",
        "      pair = (word, y)\n",
        "      if pair in freqs:\n",
        "        freqs[pair] +=1\n",
        "      else:\n",
        "        freqs[pair] = 1\n",
        "  \n",
        "  return freqs"
      ],
      "metadata": {
        "id": "5arc1h5GYfgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lookup(freqs, word, label):\n",
        "  n = 0\n",
        "  pair = (word, label)\n",
        "  if pair in freqs:\n",
        "    n = freqs[pair]\n",
        "  return n "
      ],
      "metadata": {
        "id": "VEltRSBFZQj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_pos = all_positive_tweets[:4000]\n",
        "test_pos = all_positive_tweets[4000:]\n",
        "\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "test_neg = all_negative_tweets[4000:]\n",
        "\n",
        "train_x = train_pos + train_neg\n",
        "test_x = test_pos + test_neg\n",
        "train_y = np.append(np.ones((len(train_pos))), np.zeros((len(train_neg))))\n",
        "test_y = np.append(np.ones((len(test_neg))), np.zeros((len(test_neg))))"
      ],
      "metadata": {
        "id": "3bSclFu5ZVER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freqs = count_tweets(train_x, train_y)\n",
        "\n",
        "def train_naive_bayes(freqs, train_x, train_y):\n",
        "  logliklihood = {}\n",
        "  logprior = 0\n",
        "\n",
        "  # calculate V, number of unique words in the vocabulary\n",
        "  vocab = set([pair[0] for pair in freqs.keys()])\n",
        "  V = len(vocab)\n",
        "\n",
        "  ## Calculate N_pos, N_neg, V_pos, V_neg\n",
        "  # N_pos : total number of positive words\n",
        "  # N_neg : total number of negative words\n",
        "  # V_pos : total number of unique positive words\n",
        "  # V_neg : total number of unique negative words\n",
        "\n",
        "  N_pos = N_neg = V_pos = V_neg = 0\n",
        "  for pair in freqs.keys():\n",
        "    if pair[1]>0:\n",
        "      V_pos +=1\n",
        "      N_pos += freqs[pair]\n",
        "    else:\n",
        "      V_neg +=1\n",
        "      N_neg += freqs[pair]\n",
        "\n",
        "  # Number of Documents (tweets)\n",
        "  D = len(train_y)\n",
        "\n",
        "  # D_pos, number of positive documnets\n",
        "  D_pos = len(list(filter(lambda x: x>0, train_y)))\n",
        "\n",
        "  # D_pos, number of negative documnets\n",
        "  D_neg = len(list(filter(lambda x: x<=0, train_y)))\n",
        "\n",
        "  # calculate the logprior\n",
        "  logprior = np.log(D_pos) - np.log(D_neg)\n",
        "\n",
        "  for word in vocab:\n",
        "    freqs_pos = lookup(freqs, word, 1)\n",
        "    freqs_neg = lookup(freqs, word, 0)\n",
        "\n",
        "    # calculte the probability of each word being positive and negative\n",
        "    p_w_pos = (freqs_pos+1)/(N_pos+V)\n",
        "    p_w_neg = (freqs_neg+1)/(N_neg+V)\n",
        "\n",
        "    logliklihood[word] = np.log(p_w_pos/p_w_neg)\n",
        "  \n",
        "  return logprior, logliklihood"
      ],
      "metadata": {
        "id": "ctga4gkAZfGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\n",
        "print(logprior)\n",
        "print(len(loglikelihood))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju7tRvjNbPhd",
        "outputId": "7281d55e-f049-4cd6-ead5-f25381b77550"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n",
            "9085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
        "  word_l = process_tweet(tweet)\n",
        "  p = 0\n",
        "  p+=logprior\n",
        "\n",
        "  for word in word_l:\n",
        "    if word in loglikelihood:\n",
        "      p+=loglikelihood[word]\n",
        "\n",
        "  return p"
      ],
      "metadata": {
        "id": "G_CkoBVqbXVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n",
        "  accuracy = 0\n",
        "  y_hats = []\n",
        "  for tweet in test_x:\n",
        "    if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n",
        "      y_hat_i = 1\n",
        "    else:\n",
        "      y_hat_i = 0\n",
        "    y_hats.append(y_hat_i)\n",
        "  error = np.mean(np.absolute(test_y - y_hats))\n",
        "  accuracy = 1-error\n",
        "\n",
        "  return accuracy\n",
        "  \n",
        "  print(\"Naive Bayes accuracy = %0.4f\" %\n",
        "      (test_naive_bayes(test_x, test_y, logprior, loglikelihood)))"
      ],
      "metadata": {
        "id": "RU6dQlIMbdIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great', 'I am not happy :(']\n",
        "for tweet in tweets:\n",
        "  p = naive_bayes_predict(tweet, logprior, loglikelihood)\n",
        "  if p>1:\n",
        "      print('\\033[92m')\n",
        "      print(f'{tweet} :: Positive sentiment ({p:.2f})')\n",
        "\n",
        "  else:\n",
        "\n",
        "      print('\\033[91m')\n",
        "\n",
        "      print(f'{tweet}:: Negative Sentiment ({p:.2f})')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c4c9259-f93c-4d1a-e874-36e2d77811fd",
        "id": "LxfGyzAPN7EN"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92m\n",
            "I am happy :: Positive sentiment (2.15)\n",
            "\u001b[91m\n",
            "I am bad:: Negative Sentiment (-1.29)\n",
            "\u001b[92m\n",
            "this movie should have been great. :: Positive sentiment (2.14)\n",
            "\u001b[92m\n",
            "great :: Positive sentiment (2.14)\n",
            "\u001b[92m\n",
            "great great :: Positive sentiment (4.28)\n",
            "\u001b[92m\n",
            "great great great :: Positive sentiment (6.41)\n",
            "\u001b[92m\n",
            "great great great great :: Positive sentiment (8.55)\n",
            "\u001b[91m\n",
            "I am not happy :(:: Negative Sentiment (-5.36)\n"
          ]
        }
      ]
    }
  ]
}